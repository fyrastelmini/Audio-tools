{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCunet_babble_train (1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilzvkKymYM_V"
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppLDpjEHYQD9"
      },
      "source": [
        "import os\n",
        "l1=len(os.listdir(\"./datasets/test_noisy\"))\n",
        "l2=len(os.listdir(\"./datasets/train_noisy\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWE4rznpYTjt"
      },
      "source": [
        "import argparse\n",
        "from model import *\n",
        "from model_loss import *\n",
        "from model_data import *\n",
        "from model_module import *\n",
        "from complex_layers.networks import *\n",
        "from complex_layers.STFT import *\n",
        "from complex_layers.activations import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pKKbF5IYVTZ"
      },
      "source": [
        "def data_generator(train_arguments, test_arguments):\n",
        "      train_generator = datagenerator(**train_arguments)\n",
        "\n",
        "      test_generator  = datagenerator(**test_arguments)\n",
        "      return train_generator, test_generator\n",
        "\n",
        "@tf.function\n",
        "def loop_train (model, optimizer, train_noisy_speech, train_clean_speech):\n",
        "      tf.config.experimental_run_functions_eagerly(True)\n",
        "      with tf.GradientTape() as tape:\n",
        "            train_predict_speech = model(train_noisy_speech)\n",
        "            if loss_function == \"SDR\":\n",
        "                  train_loss = modified_SDR_loss(train_predict_speech, train_clean_speech)\n",
        "            elif loss_function == \"wSDR\":\n",
        "                  train_loss = weighted_SDR_loss(train_noisy_speech, train_predict_speech, train_clean_speech)\n",
        "\n",
        "      gradients = tape.gradient(train_loss, model.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "      return train_loss\n",
        "\n",
        "@tf.function\n",
        "def loop_test (model, test_noisy_speech, test_clean_speech):\n",
        "      'Test loop do not caclultae gradient and backpropagation'\n",
        "      test_predict_speech = model(test_noisy_speech)\n",
        "      if loss_function == \"SDR\":\n",
        "            test_loss = modified_SDR_loss(test_predict_speech, test_clean_speech)\n",
        "      elif loss_function == \"wSDR\":\n",
        "            test_loss = weighted_SDR_loss(test_noisy_speech, test_predict_speech, test_clean_speech)\n",
        "\n",
        "      return test_loss\n",
        "\n",
        "def learning_rate_scheduler (epoch, learning_rate):\n",
        "      if (epoch+1) <= int(0.5*epoch):\n",
        "            return 1.00 * learning_rate\n",
        "      elif (epoch+1) > int(0.5*epoch) and (epoch+1) <= int(0.75*epoch):\n",
        "            return 0.20 * learning_rate\n",
        "      else:\n",
        "            return 0.05 * learning_rate\n",
        "      \n",
        "def model_flow (model, total_epochs, train_generator, test_generator,start=0):\n",
        "      train_step = l2// batch_size\n",
        "      test_step  = l1// batch_size\n",
        "      start_epoch=start\n",
        "\n",
        "      for epoch in tqdm(range (start_epoch,total_epochs)):\n",
        "            train_batch_losses = 0\n",
        "            test_batch_losses  = 0\n",
        "            \n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate_scheduler(epoch, learning_rate), beta_1 = 0.9)\n",
        "            'Training Loop'\n",
        "            for index, (train_noisy_speech, train_clean_speech) in tqdm(enumerate(train_generator),disable=True):\n",
        "                  loss = loop_train (model, optimizer, train_noisy_speech, train_clean_speech)\n",
        "                  train_batch_losses = train_batch_losses + loss\n",
        "\n",
        "            'Test Loop'\n",
        "            for index, (test_noisy_speech, test_clean_speech) in tqdm(enumerate(test_generator),disable=True):\n",
        "                  loss  = loop_test (model, test_noisy_speech, test_clean_speech)\n",
        "                  test_batch_losses  = test_batch_losses + loss\n",
        "\n",
        "            'Calculate loss per batch data'\n",
        "            train_loss = train_batch_losses / train_step\n",
        "            test_loss  = test_batch_losses / test_step\n",
        "           \n",
        "            templet = \"Epoch : {:3d},     TRAIN LOSS : {:.5f},     TEST LOSS  :  {:.5f}\"\n",
        "            print(templet.format(epoch+1, train_loss.numpy(), test_loss.numpy()))\n",
        "            tmplet = \"{:.5f} {:.5f}\"\n",
        "            line = tmplet.format(train_loss.numpy(), test_loss.numpy())\n",
        "            outF = open(\"loss.txt\", \"a\")\n",
        "            outF.write(line)\n",
        "            outF.write(\"\\n\")\n",
        "            outF.close()\n",
        "            if ((epoch+1) % 1) == 0: #save frequency was 10\n",
        "                  model.save(\"./model_save/\" + save_file_name + str(epoch+1) + \".h5\")\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwc5tSlXYc_L"
      },
      "source": [
        "      tf.random.set_seed(seed = 42)\n",
        "      parser = argparse.ArgumentParser(description = 'MODEL SETTING OPTION...')\n",
        "      parser.add_argument(\"--model\", type = str, default = \"dcunet20\", help = \"model type\")\n",
        "      parser.add_argument(\"--epoch\", type = int, default = 1000,        help = \"Input epochs\") #200\n",
        "      parser.add_argument(\"--batch\", type = int, default = 11,         help = \"Input batch size\") #64\n",
        "      parser.add_argument(\"--loss\",  type = str, default = \"wSDR\",     help = \"Input Loss function\") #wSDR\n",
        "      parser.add_argument(\"--optim\", type = str, default = \"adam\",     help = \"Input optimizer option\")\n",
        "      parser.add_argument(\"--lr\",    type = float, default = 0.0005,    help = \"Inputs learning rate\") #0.002\n",
        "      parser.add_argument(\"--trn\",   type = str, default = \"./datasets/train_noisy/\", help = \"training noisy\") #train_noisy5/\n",
        "      parser.add_argument(\"--trc\",   type = str, default = \"./datasets/train_clean/\", help = \"training clean\") #####change this\n",
        "      parser.add_argument(\"--ten\",   type = str, default = \"./datasets/test_noisy/\",  help = \"testing noisy\")\n",
        "      parser.add_argument(\"--tec\",   type = str, default = \"./datasets/test_clean/\",  help = \"testing clean\")\n",
        "      parser.add_argument(\"--save\",  type = str, default = \"dcunet20_\",                  help = \"save model file name\")\n",
        "      args, unknown = parser.parse_known_args()\n",
        "      model_type       = args.model\n",
        "      total_epochs     = args.epoch\n",
        "      batch_size       = args.batch\n",
        "      loss_function    = args.loss\n",
        "      optimizer_type   = args.optim\n",
        "      learning_rate    = args.lr\n",
        "      train_noisy_path = args.trn\n",
        "      train_clean_path = args.trc\n",
        "      test_noisy_path  = args.ten\n",
        "      test_clean_path  = args.tec\n",
        "      save_file_name   = args.save\n",
        "      train_arguments = {\"inputs_ids\" : os.listdir(train_noisy_path), \n",
        "                        \"outputs_ids\" : os.listdir(train_clean_path),\n",
        "                        \"inputs_dir\" : train_noisy_path, \n",
        "                        \"outputs_dir\" : train_clean_path,\n",
        "                        \"batch_size\" : batch_size}\n",
        "      test_arguments  = {\"inputs_ids\" : os.listdir(test_noisy_path), \n",
        "                        \"outputs_ids\" : os.listdir(test_clean_path),\n",
        "                        \"inputs_dir\" : test_noisy_path,\n",
        "                        \"outputs_dir\" : test_clean_path,\n",
        "                        \"batch_size\" : batch_size}\n",
        "      train_generator, test_generator = data_generator(train_arguments = train_arguments, test_arguments = test_arguments)\n",
        "      if model_type == \"naive_dcunet16\":\n",
        "            model = Naive_DCUnet16().model()\n",
        "            #custom_objects = {\"weighted_SDR_loss\": weighted_SDR_loss,\"complex_NaiveBatchNormalization\": complex_NaiveBatchNormalization,\"STFT_network\": STFT_network,\"complex_Conv2D\": complex_Conv2D,\"complex_BatchNorm2d\":complex_BatchNorm2d,\"complex_Conv2DTranspose\":complex_Conv2DTranspose,\"ISTFT_network\":ISTFT_network}\n",
        "            #with tf.keras.utils.custom_object_scope(custom_objects):\n",
        "              #model=tf.keras.models.load_model('./model_save0/ndcunet16_500.h5')\n",
        "      elif model_type == \"naive_dcunet20\":\n",
        "            model = Naive_DCUnet20().model()\n",
        "            #custom_objects = {\"weighted_SDR_loss\": weighted_SDR_loss,\"complex_NaiveBatchNormalization\": complex_NaiveBatchNormalization,\"STFT_network\": STFT_network,\"complex_Conv2D\": complex_Conv2D,\"complex_BatchNorm2d\":complex_BatchNorm2d,\"complex_Conv2DTranspose\":complex_Conv2DTranspose,\"ISTFT_network\":ISTFT_network}\n",
        "            #with tf.keras.utils.custom_object_scope(custom_objects):\n",
        "              #model=tf.keras.models.load_model('./model_save/ndcunet20_120.h5')\n",
        "      elif model_type == \"dcunet16\":\n",
        "            model = DCUnet16().model()\n",
        "      elif model_type == \"dcunet20\":\n",
        "            model = DCUnet20().model()\n",
        "            #model=tf.keras.models.load_model('./model_save/dcunet20_16.h5')\n",
        "            #custom_objects = {\"weighted_SDR_loss\": weighted_SDR_loss,\"STFT_network\": STFT_network,\"complex_Conv2D\": complex_Conv2D,\"complex_BatchNorm2d\":complex_BatchNorm2d,\"complex_Conv2DTranspose\":complex_Conv2DTranspose,\"ISTFT_network\":ISTFT_network}\n",
        "            #with tf.keras.utils.custom_object_scope(custom_objects):\n",
        "              #model=load_model_hdf5(\"model.h5\")\n",
        "              #tf.keras.models.load_model\n",
        "              #model=tf.keras.models.load_model('./model_save/dcunet20_6.h5',compile=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLZUK9d1bDgV"
      },
      "source": [
        "model.compile(optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37Y36vXtYj1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7c7b93e-7375-41e0-98c0-65f2e6b7f40f"
      },
      "source": [
        "pip install silence_tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting silence_tensorflow\n",
            "  Downloading https://files.pythonhosted.org/packages/96/d7/076b21d0e79cfc8a085f623e6577b754c50a42cfbcce51d77d0d2206988c/silence_tensorflow-1.1.1.tar.gz\n",
            "Building wheels for collected packages: silence-tensorflow\n",
            "  Building wheel for silence-tensorflow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for silence-tensorflow: filename=silence_tensorflow-1.1.1-cp37-none-any.whl size=3747 sha256=ded874d03b6b02cc062564de9938470967a3def235316413faa0ee0dc6c8db10\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/0b/35/cf3020764bee61daa81fa249df3a448e3806344a087fc12292\n",
            "Successfully built silence-tensorflow\n",
            "Installing collected packages: silence-tensorflow\n",
            "Successfully installed silence-tensorflow-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYZm5lfUYnJM"
      },
      "source": [
        "from silence_tensorflow import silence_tensorflow\n",
        "silence_tensorflow()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swqVF-bBYoc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "151ef8ec-889a-4762-c20a-386e0b8d6b3b"
      },
      "source": [
        "model_flow (model, total_epochs, train_generator, test_generator,7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/993 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZKCSjA9mCYc",
        "outputId": "8c70dc8c-c917-4a81-f1ff-4a055ae73d7b"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jun  6 11:54:24 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
